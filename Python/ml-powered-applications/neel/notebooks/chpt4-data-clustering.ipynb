{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Powered Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "Data exploration notebook to better understand the data.\n",
    "\n",
    "Objective\n",
    "- To label and identify trends\n",
    "\n",
    "Process\n",
    "- Generate summary statistics\n",
    "- Identifying differences in class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "en-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you'll have spacy 3.0.1 which is incompatible.\n",
      "en-core-web-lg 2.3.1 requires spacy<2.4.0,>=2.3.0, but you'll have spacy 3.0.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading spacy-3.0.1-cp38-cp38-win_amd64.whl (11.8 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy_legacy-3.0.1-py2.py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Collecting srsly<3.0.0,>=2.4.0\n",
      "  Downloading srsly-2.4.0-cp38-cp38-win_amd64.whl (451 kB)\n",
      "Requirement already satisfied, skipping upgrade: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Collecting catalogue<2.1.0,>=2.0.1\n",
      "  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.3-cp38-cp38-win_amd64.whl (1.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Collecting thinc<8.1.0,>=8.0.0\n",
      "  Downloading thinc-8.0.1-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting pathy\n",
      "  Downloading pathy-0.3.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107101 sha256=54223bdeae34ffc0974edae645bbafb59afd85d190289e75903d894d8821397d\n",
      "  Stored in directory: c:\\users\\nrosh\\appdata\\local\\pip\\cache\\wheels\\11\\73\\9a\\f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "Successfully built smart-open\n",
      "Installing collected packages: spacy-legacy, catalogue, srsly, wasabi, pydantic, thinc, typer, smart-open, pathy, spacy\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 0.8.0\n",
      "    Uninstalling wasabi-0.8.0:\n",
      "      Successfully uninstalled wasabi-0.8.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.5\n",
      "    Uninstalling thinc-7.4.5:\n",
      "      Successfully uninstalled thinc-7.4.5\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 4.1.0\n",
      "    Uninstalling smart-open-4.1.0:\n",
      "      Successfully uninstalled smart-open-4.1.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.3.5\n",
      "    Uninstalling spacy-2.3.5:\n",
      "      Successfully uninstalled spacy-2.3.5\n",
      "Successfully installed catalogue-2.0.1 pathy-0.3.6 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.1 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.1 typer-0.3.2 wasabi-0.8.2\n",
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn) (0.23.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: numba>=0.49 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn) (0.51.2)\n",
      "Requirement already satisfied, skipping upgrade: pynndescent>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from umap-learn) (0.5.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: llvmlite<0.35,>=0.34.0.dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from numba>=0.49->umap-learn) (50.3.1.post20201107)\n",
      "Building wheels for collected packages: umap-learn\n",
      "  Building wheel for umap-learn (setup.py): started\n",
      "  Building wheel for umap-learn (setup.py): finished with status 'done'\n",
      "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76570 sha256=8cb86c3fd29e781a336713980616a0a371a06dca4ff8a9808e8ca84403dbaa12\n",
      "  Stored in directory: c:\\users\\nrosh\\appdata\\local\\pip\\cache\\wheels\\95\\85\\b7\\b4b7040e49367b6d1505d7e8fb57e3e79b22fa6ac26f72520b\n",
      "Successfully built umap-learn\n",
      "Installing collected packages: umap-learn\n",
      "  Attempting uninstall: umap-learn\n",
      "    Found existing installation: umap-learn 0.5.0\n",
      "    Uninstalling umap-learn-0.5.0:\n",
      "      Successfully uninstalled umap-learn-0.5.0\n",
      "Successfully installed umap-learn-0.5.1\n",
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: pathy in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 2.3.1\n",
      "    Uninstalling en-core-web-sm-2.3.1:\n",
      "      Successfully uninstalled en-core-web-sm-2.3.1\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "# run script on a 3.6 environment - base36\n",
    "!pip install -U spacy\n",
    "!pip install -U umap-learn\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# intialization\n",
    "PATH_data = r\"C:\\Users\\nrosh\\Desktop\\Personal Coding Projects\\Python\\ml-powered-applications\\neel\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "1. Clustering\n",
    "    - Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)\n",
    "    - Many clustering algorithms group data points by measuring the distance between points and assigning ones that are close to each other to the same cluster.\n",
    "    - The vast majority of datasets can be separated into clusters based on their features, labels, or a combination of both. Examining each cluster individually and the similarities and differences between clusters is a great way to identify structure in a dataset.\n",
    "    - Clustering algorithms work on vectors, so we canâ€™t simply pass a set of sentences to a clustering algorithm. To get our data ready to be clustered, we will first need to vectorize it.\n",
    "\n",
    "2. Vectorization\n",
    "    - A process of converting a raw data set into a singular or multi-dimensional vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization techniques\n",
    "\n",
    "Clustering requires distances to be measured on the \"same\" scale.\n",
    "\n",
    "The approach for vectorizing/normalizing data depends on the structure and type of data being analysed.\n",
    "\n",
    "1. Tabular data\n",
    "    - Continuous features should be normalized to a common scale\n",
    "    - Categorical features such as colors can be converted to a one-hot encoding (binary transformations). This allows the distance between points to always remain the same.\n",
    "    \n",
    "2. Text data\n",
    "\n",
    "    - Bag Of Words (Tokenize sentences and count their observations by row)\n",
    "        - The simplest way to vectorize text is to use a count vector, which is the word equivalent of one-hot encoding.\n",
    "        - For each sentence, the number at each index represents the count of occurrences of the associated word in the given sentence.\n",
    "        -This method ignores the order of the words in a sentence\n",
    "        - scikit-learn TfidfVectorizer\n",
    "            - Produce a vector of tokenized words for count aggregation by row\n",
    "            - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "    - Word2ec and fastText\n",
    "        - These vectorization techniques produce word vectors that attempt to learn a representation that captures similarities between concepts better than a TF-IDF encoding. \n",
    "        - They do this by learning which words tend to appear in similar contexts in large bodies of text such as Wikipedia.\n",
    "        - This approach is based on the distributional hypothesis, which claims that linguistic items with similar distributions have similar meanings.\n",
    "            - This is done by learning a vector for each word and training a model to predict a missing word in a sentence using the word vectors of words around it. \n",
    "            - The number of neighboring words to take into account is called the window size.\n",
    "    - Dimensionality Reduction\n",
    "        - Vectorized data are multi-dimensional and can't be visualized\n",
    "        - The goal is to use a method that reduces multidimensional data into a visual space whilst minimizing the data loss associated with dimensional reduction\n",
    "        - Techniques\n",
    "            - t-SNE\n",
    "            - UMAP\n",
    "        - These techniques are useful for to notice patterns in data on a very high level\n",
    "        - The goal is to use these methods to see whether there are regions of the data that can easily be seperated by a production model\n",
    "        - \n",
    "\n",
    "UMAP\n",
    "    - General purpose manifold learning and dimension reduction algorithm\n",
    "    \n",
    "    \n",
    "Once we have a vectorized representation of our unstructured data, we can use it for the purpose of data inspection/exploration or outcome predictions.\n",
    "\n",
    "1. Inspection\n",
    "        - Dimensionality Reduction\n",
    "             - Vectors produced from unstructured data often have more than one dimension. The dataset needs to be reduced in some way for us to visualize it on a two-dimensional plane.\n",
    "             - \n",
    "\n",
    "2. Prediction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling Strategy\n",
    "Feel free to update your vectorization strategy by adding any features you discover to help make your data representation as informative as possible, and go back to labeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"vectorization_strategy.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"vectorization_strategy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics Observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Are there distinct regions of post's title that can be classified into one or multiple labels?\n",
    "    \n",
    "Sources:\n",
    "- stackexchange: \n",
    "    - Data Schema: https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
    "    - Score: https://meta.stackexchange.com/questions/229255/what-is-the-score-of-a-post\n",
    "    - UMAP: https://umap-learn.readthedocs.io/en/latest/\n",
    "    \n",
    "- word2vec: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df(df, cols, **kwargs):\n",
    "\n",
    "    # rename and return a dataframe of those columns\n",
    "    # choose to export or not\n",
    "    \n",
    "    _df = df.loc[:, cols]\n",
    "    \n",
    "    # rename dict\n",
    "    if \"rename_dict\" in kwargs.keys() :\n",
    "        _df.rename(columns=kwargs[\"rename_dict\"], inplace=True)\n",
    "        print('- Columns renamed.')\n",
    "    \n",
    "    # export data\n",
    "    if kwargs[\"export_loc\"]:\n",
    "        \n",
    "        # handle data export\n",
    "        try:\n",
    "            \n",
    "            if \"export_name\" in kwargs.keys():\n",
    "                _location = kwargs[\"export_loc\"] + \"\\\\{}\".format(kwargs[\"export_name\"]) + \".csv\"\n",
    "                _df.to_csv(_location)\n",
    "                print(f\"\"\"- File exported to: {_location}\"\"\")\n",
    "            else:\n",
    "                _location = kwargs[\"export_loc\"]+\"\\\\adhoc_{}\".format(datetime.today().strftime(\"%m%d%y\"))+\".csv\"\n",
    "                _df.to_csv(_location)\n",
    "                print(f\"\"\"- File exported to: {_location}\"\"\")\n",
    "\n",
    "        except:\n",
    "            raise Exception(f\"\"\"export_loc must be of type str. Given: {type(kwargs[\"export_loc\"])}\"\"\")\n",
    "    \n",
    "    print('\\n')\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42605 entries, 0 to 42604\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   PostTypeId            42605 non-null  int64  \n",
      " 1   AcceptedAnswerId      5081 non-null   float64\n",
      " 2   ParentId              31935 non-null  float64\n",
      " 3   AnswerCount           9935 non-null   float64\n",
      " 4   CommentCount          42605 non-null  int64  \n",
      " 5   FavoriteCount         4052 non-null   float64\n",
      " 6   LastActivityDate      42605 non-null  object \n",
      " 7   CreationDate          42605 non-null  object \n",
      " 8   ClosedDate            1294 non-null   object \n",
      " 9   LastEditDate          15252 non-null  object \n",
      " 10  Score                 42605 non-null  int64  \n",
      " 11  Title                 9935 non-null   object \n",
      " 12  body_text             42519 non-null  object \n",
      " 13  fe_tenure             42605 non-null  float64\n",
      " 14  fe_isclosed           42605 non-null  int64  \n",
      " 15  fe_isquestion         42605 non-null  int64  \n",
      " 16  fe_isanswer           42605 non-null  int64  \n",
      " 17  fe_isfavorited        42605 non-null  int64  \n",
      " 18  fe_wasedited          42605 non-null  int64  \n",
      " 19  fe_question_answered  42605 non-null  int64  \n",
      "dtypes: float64(5), int64(9), object(6)\n",
      "memory usage: 6.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "df_orig = pd.read_csv(\n",
    "    PATH_data + \"\\\\out\\cleaned_data.csv\",\n",
    ")\n",
    "\n",
    "# copies\n",
    "df = df_orig.iloc[:, 1::].copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "\n",
    "def cosine_sim(X, Y):\n",
    "    \n",
    "    # function to aggregate two vectors into a real number that represents how close these vectors are\n",
    "    # Source: https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "    \n",
    "    cs1 = np.sum([X[i]*Y[i] for i in range(len(X))])\n",
    "    cs2 = (np.sum(np.power(X, 2))**0.5)*(np.sum(np.power(Y, 2))**0.5)\n",
    "    cs = cs1/cs2\n",
    "\n",
    "    return cs, cs1, cs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        --\n",
      "        \n",
      "        Given,\n",
      "            X: [9, 0, 2, 2, 0, 6, 8, 0, 0, 9]\n",
      "            Y: [4, 8, 7, 9, 1, 8, 8, 1, 0, 7]\n",
      "\n",
      "        Cosine Similarity: \n",
      "\n",
      "            cs: 0.75\n",
      "            cs1: 243\n",
      "            cs2: 324.08\n",
      "        \n",
      "    \n",
      "\n",
      "        --\n",
      "        \n",
      "        Given,\n",
      "            X: [-0.4, 0.8]\n",
      "            Y: [-0.3, 0.2]\n",
      "\n",
      "        Cosine Similarity: \n",
      "\n",
      "            cs: 0.87\n",
      "            cs1: 0.28\n",
      "            cs2: 0.32\n",
      "        \n",
      "    \n",
      "\n",
      "        --\n",
      "        \n",
      "        Given,\n",
      "            X: [-0.4, 0.8]\n",
      "            Y: [-0.5, -0.4]\n",
      "\n",
      "        Cosine Similarity: \n",
      "\n",
      "            cs: -0.21\n",
      "            cs1: -0.12\n",
      "            cs2: 0.57\n",
      "        \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# cosine similarity of two vectors\n",
    "\n",
    "mast_A = [\n",
    "    [rand.randrange(0, 10) for i in range(10)],\n",
    "    [-0.4, 0.8],\n",
    "    [-0.4, 0.8]\n",
    "]\n",
    "\n",
    "mast_B = [\n",
    "    [rand.randrange(0, 10) for i in range(10)],\n",
    "    [-0.3, 0.2],\n",
    "    [-0.5, -0.4]\n",
    "]\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    print(f\"\"\"\n",
    "        --\n",
    "        \n",
    "        Given,\n",
    "            X: {mast_A[i]}\n",
    "            Y: {mast_B[i]}\n",
    "\n",
    "        Cosine Similarity: \n",
    "\n",
    "            cs: {np.round(cosine_sim(mast_A[i], mast_B[i])[0], 2)}\n",
    "            cs1: {np.round(cosine_sim(mast_A[i], mast_B[i])[1], 2)}\n",
    "            cs2: {np.round(cosine_sim(mast_A[i], mast_B[i])[2], 2)}\n",
    "        \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\util.py:707: UserWarning: [W095] Model 'en_core_web_lg' (2.3.1) requires spaCy >=2.3.0,<2.4.0 and is incompatible with the current version (3.0.1). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config.cfg from C:\\ProgramData\\anaconda3\\lib\\site-packages\\en_core_web_lg\\en_core_web_lg-2.3.1\\config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6cd8d547a658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Returns a Language Object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnlp\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"parser\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tagger\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"textcat\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Total word vectors:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, disable, exclude, config)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"blank:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# installed as package\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# path to model data directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \"\"\"\n\u001b[0;32m    354\u001b[0m     \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\en_core_web_lg\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m     return load_model_from_path(\n\u001b[0m\u001b[0;32m    515\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_config\u001b[1;34m(path, overrides, interpolate)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE053\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"config.cfg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m         return config.from_disk(\n\u001b[0;32m    547\u001b[0m             \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E053] Could not read config.cfg from C:\\ProgramData\\anaconda3\\lib\\site-packages\\en_core_web_lg\\en_core_web_lg-2.3.1\\config.cfg"
     ]
    }
   ],
   "source": [
    "# Use tsne to output GloVe word embeddings\n",
    "# source: https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-glove.html\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Returns a Language Object\n",
    "nlp        = spacy.load('en_core_web_lg', disable=[\"parser\", \"tagger\", \"ner\", \"textcat\"])\n",
    "\n",
    "print('Total word vectors:', len(nlp.vocab.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings - Dark bands represent positively higher similarities\n",
    "from matplotlib import cm\n",
    "\n",
    "# words of curiosity\n",
    "post_titles = pd.Series([\n",
    "    \"king\", \"Man\", \n",
    "    \"Woman\", \"queen\"\n",
    "])\n",
    "\n",
    "# get embedding and formalize\n",
    "spacy_vectors  = post_titles.apply(lambda x: nlp(x).vector)\n",
    "title_emb = dict(zip(post_titles, spacy_vectors)) \n",
    "\n",
    "print(f\"\"\" \n",
    "    \n",
    "    \n",
    "    Similarity spectrum of word embeddings for a list of words.\n",
    "\"\"\")\n",
    "\n",
    "# iterate between words in a pandas series\n",
    "for i in range(len(post_titles)):\n",
    "    \n",
    "    # visualization parameters\n",
    "    colors = cm.gist_yarg(spacy_vectors[i] / float(max(spacy_vectors[i]))) # color vector to hue the bars\n",
    "    x = range(len(spacy_vectors[0])) # position of each bar\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=[20, 1])\n",
    "    fig.gca().set_ylabel(post_titles.loc[i], fontsize=\"xx-large\")\n",
    "    \n",
    "    # plot the similairy spectrum for each word\n",
    "    bar = plt.bar(\n",
    "        x,\n",
    "        1,\n",
    "        1,\n",
    "        color=colors,\n",
    "        figure=fig\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies: King - man + woman\n",
    "\n",
    "Apply mathematical operations to approximate deductive reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#links\n",
    "\n",
    "from gensim import models\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained word2vec model\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline entities to infer analogy\n",
    "baseline_entities = {\n",
    "    \"positive\": [\"king\", \"queen\"],\n",
    "    \"negative\": [\"man\"]\n",
    "}\n",
    "\n",
    "most_similar = wv.most_similar(\n",
    "    positive=baseline_entities[\"positive\"], \n",
    "    negative=baseline_entities[\"negative\"]\n",
    ")\n",
    "\n",
    "title_emb[\"king-man+woman\"] = spacy_vectors[0] - spacy_vectors[1] + spacy_vectors[2] \n",
    "\n",
    "print(title_emb.keys())\n",
    "pd.DataFrame(data=most_similar, columns=[\"Word\", \"Similarity Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate entity vectors for words closest to\n",
    "post_titles = pd.Series([\n",
    "    \"queens\", \"kings\", \"monarch\", \"royals\"\n",
    "])\n",
    "\n",
    "# get embedding and formalize\n",
    "spacy_vectors  = post_titles.apply(lambda x: nlp(x).vector)\n",
    "similarity_emb = dict(zip(post_titles, spacy_vectors)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [\"king-man+woman\", \"king\", \"Man\", \"Woman\"]:\n",
    "    \n",
    "    # visualization parameters\n",
    "    colors = cm.gist_yarg(title_emb[k] / float(max(title_emb[k]))) # color vector to hue the bars\n",
    "    x = range(len(title_emb[\"king\"])) # position of each bar\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=[20, 1])\n",
    "    fig.gca().set_ylabel(k, fontsize=\"xx-large\")\n",
    "    \n",
    "    # plot the similairy spectrum for each word\n",
    "    bar = plt.bar(\n",
    "        x,\n",
    "        1,\n",
    "        1,\n",
    "        color=colors,\n",
    "        figure=fig\n",
    "    )\n",
    "    \n",
    "    break # for result only\n",
    "    \n",
    "for k in post_titles:\n",
    "    \n",
    "    # visualization parameters\n",
    "    colors = cm.gist_yarg(similarity_emb[k] / float(max(similarity_emb[k]))) # color vector to hue the bars\n",
    "    x = range(len(similarity_emb[\"queens\"])) # position of each bar\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=[20, 1])\n",
    "    fig.gca().set_ylabel(k, fontsize=\"xx-large\")\n",
    "    \n",
    "    # plot the similairy spectrum for each word\n",
    "    bar = plt.bar(\n",
    "        x,\n",
    "        1,\n",
    "        1,\n",
    "        color=colors,\n",
    "        figure=fig\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Dimensionality Reduction\n",
    "Using UMAP and Clustering to reduce the dimensional plane of the word embeddings such that they can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# construct embeddings for the questions listed on the post titles\n",
    "post_titles = df.loc[df[\"PostTypeId\"] == 1, \"Title\"]\n",
    "post_title_vectors  = post_titles.apply(lambda x: nlp(x).vector)\n",
    "\n",
    "df_emb = pd.DataFrame(\n",
    "    {\n",
    "        \"PrimaryID\"          : df.loc[df[\"PostTypeId\"] == 1, \"Title\"].index,\n",
    "        \"post_title\"         : post_titles.to_list(),\n",
    "        \"vector_embedding\"   : post_title_vectors,\n",
    "        \"question_answered\"  : df.loc[df[\"PostTypeId\"]==1, \"fe_question_answered\"]\\\n",
    "                                 .apply(lambda row: True if row==1 else False)\n",
    "    }\n",
    ")\n",
    "\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### umap to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "\n",
    "# initialization\n",
    "n_clusters=3\n",
    "\n",
    "# Fit UMAP to our data, and return the transformed data\n",
    "reducer = umap.UMAP()\n",
    "umap = reducer.fit_transform(post_title_vectors.to_list())\n",
    "\n",
    "# cluster the data using K Means\n",
    "clus = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "clusters = clus.fit_predict(post_title_vectors.to_list())\n",
    "\n",
    "# check to see that the length of the original is the same as reduced\n",
    "print(f\"\"\"\n",
    "   \n",
    "   original: {len(post_title_vectors.to_list())}\n",
    "   reduced:  {len(umap)}\n",
    "   \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for seaborn visualization\n",
    "import seaborn as sns\n",
    "\n",
    "df_dim = pd.DataFrame(\n",
    "    {\n",
    "        \"PrimaryId\"          : df_emb.PrimaryID,\n",
    "        \"post_title\"         : df_emb.post_title,\n",
    "        \"x\"                  : umap[:, 0],\n",
    "        \"y\"                  : umap[:, 1],\n",
    "        \"question_answered\"  : df_emb.question_answered,\n",
    "        \"cluster\"            : clusters\n",
    "    }\n",
    ")\n",
    "\n",
    "# differentiate between answered questions\n",
    "df_answered = df_dim.loc[df_dim.question_answered == 1]\n",
    "df_nanswered = df_dim.loc[df_dim.question_answered == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_loc = PATH_data+\"\\\\labelling\"\n",
    "\n",
    "# columns to select\n",
    "cols_s = [\n",
    "    'PrimaryId', 'post_title', \n",
    "    'x', 'y', \n",
    "    'question_answered', 'cluster',\n",
    "]\n",
    "\n",
    "# export and display df info\n",
    "export_df(df_dim, cols_s, export_loc=export_loc, export_name=\"questions_by_cluster\").info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# visualize results\n",
    "fig = plt.figure(figsize=(24, 18))\n",
    "cmap = plt.get_cmap(\"Set2\")\n",
    "\n",
    "color_map = {\n",
    "    True: '#ff7f0e',\n",
    "    False:'#1f77b4'\n",
    "}\n",
    "\n",
    "# Plot umap clusters\n",
    "ax1 = plt.subplot(221)\n",
    "umap_answered = sns.scatterplot(\n",
    "    x=df_answered.x,\n",
    "    y=df_answered.y,\n",
    "    c=[color_map[True] for i in range(0, len(df_answered))],\n",
    "    axes=ax1,\n",
    "    s=40, \n",
    "    alpha=.4,\n",
    "    label=\"Answered\"\n",
    ")\n",
    "\n",
    "umap_nanswered = sns.scatterplot(\n",
    "    x=df_nanswered.x,\n",
    "    y=df_nanswered.y,\n",
    "    c=[color_map[False] for i in range(0, len(df_nanswered))],\n",
    "    axes=ax1,\n",
    "    s=40, \n",
    "    alpha=.4,\n",
    "    label=\"Unanswered\"\n",
    ")\n",
    "\n",
    "ax1.set_title(\"UMAP reduction by question_answered\")\n",
    "ax1.legend(loc=3)\n",
    "\n",
    "# K-Mean clusters\n",
    "ax_3 = plt.subplot(223)\n",
    "kmean_clus = plt.scatter(\n",
    "    df_dim.x, \n",
    "    df_dim.y, \n",
    "    c=[cmap(x/n_clusters) for x in clusters], \n",
    "    s=40, \n",
    "    alpha=.4\n",
    ")\n",
    "ax_3.set_title(\" {} kMean clusters\".format(len(np.unique(clusters))))\n",
    "\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Optimizing for Clusters\n",
    "\n",
    "1. Sources\n",
    " - Selecting the optimal number of clusters, https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "2. Silhouette coefficients\n",
    " - near +1 indicates that the sample is far away from the neighboring clusters. \n",
    " - 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    " - The further away a point is from it's decision boundary, the more likely that is part of it.\n",
    " - The score determines the average distance of all points from their decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "# This gives a perspective into the density and separation of the formed clusters\n",
    "\n",
    "# initialization\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "_clusters = []\n",
    "_silscores = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # perform clustering operations\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(post_title_vectors.to_list())\n",
    "    \n",
    "    # determine silhouette_avg scores\n",
    "    silhouette_avg = silhouette_score(post_title_vectors.to_list(), cluster_labels, metric='cosine')\n",
    "    \n",
    "    # save data\n",
    "    _clusters.append(n_clusters) \n",
    "    _silscores.append(silhouette_avg)\n",
    "    \n",
    "# evaluate performance\n",
    "df_clusperf = pd.DataFrame(\n",
    "    data={\n",
    "        \"clusters\": _clusters,\n",
    "        \"silhoutte_scores\": _silscores\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimal number of clusters by determining the average distance all points are from their decision boundaries\n",
    "optimal_clusters = df_clusperf.sort_values(\"silhoutte_scores\", ascending=False).loc[0, \"clusters\"]\n",
    "\n",
    "df_clusperf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YAMLHandler:\n",
    "    \n",
    "    # class to handle reading and writing yaml files to location\n",
    "    yaml = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def read(self, read_loc):\n",
    "        \n",
    "        # read_loc str\n",
    "        \n",
    "        self.read_loc = read_loc\n",
    "        try:\n",
    "            with open(read_loc, 'r') as stream:\n",
    "                return yaml.safe_load(stream)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        pass\n",
    "\n",
    "    def write(self, write_loc, data):\n",
    "        \n",
    "        # write_loc str\n",
    "        # data json\n",
    "        \n",
    "        try:\n",
    "            with io.open(write_loc, 'w', encoding='utf8') as outfile:\n",
    "                yaml.dump(\n",
    "                    data, \n",
    "                    outfile, \n",
    "                    default_flow_style=False, \n",
    "                    allow_unicode=True\n",
    "                )\n",
    "            print('Data written to {}'.format(write_loc))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import yaml\n",
    "import io\n",
    "\n",
    "# save model onto back end\n",
    "clf = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "clf_labels = clusterer.fit_predict(post_title_vectors.to_list())\n",
    "\n",
    "# save conditions\n",
    "model_params = {\n",
    "    \"model_location\" : '..\\pretrained_models\\kmeans_optimal.joblib',\n",
    "    \"yaml_location\" : '..\\pretrained_models\\kmeans_optimal.yaml',\n",
    "    \"opt_clusters\"  : str(optimal_clusters)\n",
    "}\n",
    "\n",
    "# save model\n",
    "dump(clf, model_params[\"model_location\"]) \n",
    "\n",
    "# save model metadata\n",
    "YAML = YAMLHandler()\n",
    "YAML.write(model_params[\"yaml_location\"], model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model and produce new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and check for equality\n",
    "loc_modelparams = '..\\pretained_models\\kmeans_optimal.yaml'\n",
    "\n",
    "kmeans_params = YAML.read(loc_modelparams)\n",
    "kmeans_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word embeddings\n",
    "post_title_vectors.to_list()\n",
    "\n",
    "# load the model and generate cluster predictions\n",
    "clf_loaded = load(kmeans_params['model_location'])\n",
    "clf_labels_loaded = clusterer.fit_predict(post_title_vectors.to_list()) \n",
    "\n",
    "# check for equality\n",
    "_data = {}\n",
    "_data[True] = 0\n",
    "_data[False] = 0\n",
    "\n",
    "for val in (clf_labels == clf_labels_loaded):\n",
    "    _data[val] += 1\n",
    "    \n",
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
